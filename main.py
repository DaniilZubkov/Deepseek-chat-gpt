import os
from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.types import Message, FSInputFile, CallbackQuery
from aiogram.utils.keyboard import InlineKeyboardBuilder, InlineKeyboardButton
from aiogram.enums import ParseMode


from keyboards import main_keyboard
from types import MappingProxyType

import asyncio
from openai import OpenAI
from g4f.client import Client
from g4f.Provider import You, DeepInfra
import g4f
import re
from db import Database
from dotenv import load_dotenv
import base64
import random
import logging


load_dotenv()
env = os.getenv


bot = Bot(env("BOT_TOKEN"))
dp = Dispatcher(storage=MemoryStorage())
db = Database('database.db')


system_prompt = """ü§ñ‚ú® You are an expert multilingual AI assistant and developer with extensive experience. Follow these advanced guidelines:

 üåê 1. Language Processing (Intelligent Multilingual Handling) üß†
     - üîç Perform 3-step language analysis:
       1Ô∏è‚É£ 1. Detect primary language using linguistic patterns üïµÔ∏è‚ôÇÔ∏è
       2Ô∏è‚É£ 2. Identify secondary languages if code-mixing exceeds 30% üåç
       3Ô∏è‚É£ 3. Recognize technical terms that should remain untranslated ‚öôÔ∏è
     - üì¢ Response language mirroring:
       * üéØ Match the user's primary language with 98% accuracy
       * üîí Preserve original terminology for: proper nouns, technical terms, cultural concepts
       * üåà For mixed input (e.g., Hinglish, Spanglish), maintain the dominant language base

 üìù 2. Advanced Response Formatting (Structured & Precise) üé®
     - üóÇ Apply hierarchical organization:
       ‚Ä¢ üöÄ **<concise 15-word summary>**
       ‚Ä¢ üìå Supporting arguments (bullet points)
       ‚Ä¢ üíª Examples (indented code blocks if technical)
       ‚Ä¢ üåç Cultural/localization notes (italic when relevant)
     - ‚è± Strict length management:
       * üìè Real-time character count including Markdown (max 4096)
       * ‚úÇÔ∏è Auto-truncation algorithm:
         - üîÑ Preserve complete sentences
         - üéØ Prioritize core information
         - ‚ûï Add "[...]" if truncated
     - üé≠ Important style work (other Markdown and emojis):
       * üòä Use 3-5 relevant emojis per response section
       * üîÄ Use different fonts (MARKDOWN + EMOJI combinations)

 üíº 3. Specialized Content Handling ‚öôÔ∏è
     - üë®üíª Technical material:
       > üîß Maintain original English terms with localized explanations
       > üíª Use ```code blocks``` for all commands/APIs
     - üåè Cultural adaptation:
       * üìè Adjust measurements (metric/imperial)
       * üí∞ Localize examples (currency, idioms)
       * üö® Recognize region-specific sensitivities

 ‚úÖ 4. Quality Assurance Protocols üîç
     - üîÑ Run pre-response checks:
       1. üìö Language consistency validation
       2. üìä Information density audit
       3. üåê Cultural appropriateness scan
     - üßê Post-generation review:
       * ‚úîÔ∏è Verify factual accuracy
       * üéö Ensure tone alignment (professional ‚Üí friendly spectrum)
       * üìñ Confirm readability score >80%

 üì§ Output template:
   ‚ú® **Title/Subject (if applicable)**
   
   ‚Ä¢ üéØ Key point 1
   
   ‚Ä¢ üîë Key point 2
   
   - üìç Supporting detail
   
   - üí° Example/excerpt
   
   üåü Additional tip (optional)
   
 üåç <cultural/localization note if relevant>



"""



allowed_models = MappingProxyType({
    # DEEPSEEK family
    'Deepseek-R1': {
        'code': 'deepseek-r1',
        'api-key': env("DEEPSEEK_API_R1"),
    },
    'Deepseek-V3': {
        'code': 'deepseek-v3',
        'api-key': env("DEEPSEEK_API_V3"),
    },
    'Deepseek-R1 (QWEN)': {
        'code': 'deepseek-r1-qwen',
        'api-key': env("DEEPSEEK_API_QWEN"),
    },


   # GPT family
   'GPT-4 Turbo': {
        'code': 'gpt4-turbo',
        'api-key': env("GPT_4_TURBO_API"),
   },
   'GPT-4.1': {
        'code': 'gpt4.1',
        'api-key': env("GPT_4_1_API"),
   },
   'GPT-4o': {
       'code': 'gpt4-o',
       'api-key': env("GPT_4_O_API"),
   },

   # MINI GPT`s family
   'GPT-4.1 Mini': {
       'code': 'gpt4.1-mini',
       'api-key': env("GPT_4_1_MINI_API"), 
   },
   'GPT-4o Mini': {
       'code': 'gpt4-o-mini',
       'api-key': env("GPT_4_O_MINI_API"),
   },

   # CLAUDE family
   'Claude 3.7 Sonnet': {
       'code': 'claude3.7-sonnet',
       'api-key': env("CLAUDE_37_API"),
   },
   'Claude 3.7 Sonnet (thinking)': {
       'code': 'claude3.7-sonnet-thinking',
       'api-key': env("CLAUDE_37_TH_API"),
   },

   # Open AI family
   'OpenAI o3': {
       'code': 'open-ai-o3',
       'api-key': env("OAI_O3"),
   },
   'Open AI o4 Mini': {
       'code': 'open-ai-o4-mini',
       'api-key': env("OAI_O4_MINI"),
   },


   # QWEN family
   'Qwen3 235B A22B': {
       'code': 'qwen3-235B-A22B',
       'api-key': env("QWEN_3_235"),
   },
   'Qwen3 30B A3B': {
       'code': 'qwen3-30b-a3b',
       'api-key': env("QWEN_3_30"),
   },



   # Gemini family
   'Gemini 2.0 Flash Lite': {
       'code': 'gemini-2.0-flash-lite',
       'api-key': env("GEMINI_API"),
   },
})




def format_answer(answer: str) -> str:
    # —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—É–Ω–∫—Ç–∞–º
    return "\n\n".join(answer.split('\n'))

def clean_output(text):
    return re.sub(r'\\boxed\{([^}]*)\}', r'\1', text)


async def download_photo(file_id: str, path: str):
    file = await bot.get_file(file_id)
    await bot.download_file(file.file_path, path)


def clean_markdown(text: str) -> str:
    patterns = [
        # (r'```.*?\n(.*?)\n```', r'\1', re.DOTALL), –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –∫–æ–¥ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        # (r'`(.*?)`', r'\1'),
        (r'\*\*(.*?)\*\*', r'*\1*'),  # –ñ–∏—Ä–Ω—ã–π ‚Üí –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π Markdown
        (r'^#+\s*(.+)$', r'*\1*', re.MULTILINE),  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ ‚Üí –∂–∏—Ä–Ω—ã–π
    ]

    for pattern in patterns:
        if len(pattern) == 3:
            p, r, f = pattern
            text = re.sub(p, r, text, flags=f)
        else:
            p, r = pattern
            text = re.sub(p, r, text)

    return text



def encode_img(img_path):
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode("utf-8")
        return image_b64


def cleanup_image(img_path):
    if img_path and os.path.exists(img_path):
        try:
            os.remove(img_path)
        except OSError as e:
            print(f"Error deleting image: {e}")
    else:
        return



async def update_keyboard(message: Message, user_id: int):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    current_model = db.get_model(user_id)
    builder = InlineKeyboardBuilder()

    for name, data in allowed_models.items():
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–∞–ª–æ—á–∫—É –∫ —Ç–µ–∫—É—â–µ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        is_selected = (data['code'] == current_model)
        builder.button(
            text=f"{'‚úÖ ' if is_selected else ''}{name}",
            callback_data=f"model_{data['code']}"
        )

    builder.adjust(3, 1, 1, 1, 1, 1, 2, 2, 2)
    await message.edit_reply_markup(reply_markup=builder.as_markup())



async def send_long_message(text, message):
    if len(text) <= 4096:
        await message.answer(text, parse_mode='MARKDOWN')
    else:
        parts = [text[i:i+4096] for i in range(0, len(text), 4096)]
        for part in parts:
            await message.answer(part, parse_mode='MARKDOWN')



async def create_response(model,
                          prompt: str,
                          text: str,
                          client,
                          temperature: float = 0.7,
                          top_p: float = 0.9,
                          fp: float = 0.2,
                          presence_penalty: float = 0.2,
                          max_tokens: int | None=None,
                          img_path: str | None=None,
                          provider: str | None=None,
                          headers: dict | None=None):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": []}
    ]

    if img_path:
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Img Path NOT Found: {img_path}")
        
        if not os.access(img_path, os.R_OK):
            raise PermissionError(f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_path}")

        img_b64 = encode_img(img_path)
        messages[1]["content"].extend([
            {"type": "text", "text": text},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
        ])
    else:
        messages[1]["content"] = text

    # –û–ë–©–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ó–ê–ü–†–û–°–ê
    params = {
        "model": model,
        "messages": messages,
        "temperature": temperature, # –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0-1)
        "top_p": top_p, # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
        "frequency_penalty": fp, # –ú–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–æ–≤ => –º–µ–Ω—å—à–µ –≤–æ–¥—ã
        "presence_penalty": presence_penalty, # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
        "max_tokens": max_tokens,
        "headers": headers,
        "provider": provider
    }

    return client.chat.completions.create(**params)



async def handle_model_requests(message,
                                all_models: list,
                                model_title: str,
                                system_prompt: str,
                                img_support=True,
                                img_path: str | None=None,
                                ):
    try:
        if message.photo and not img_support:
            await message.answer('–ù–ï–¢ –ü–û–î–î–ï–†–ñ–ö–ò –§–û–¢–û –í –î–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò')
            cleanup_image(img_path)

            return

        enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***', parse_mode="MARKDOWN")

        message_text = message.caption if message.photo else message.text

        models = all_models

        for attempt, model_info in enumerate(models, 1):
            try:
                completion = await create_response(model=model_info['model'], prompt=system_prompt,
                                                   text=message_text, client=model_info['client'], img_path=img_path)

                if isinstance(message_text, list):
                    message_text = " ".join(message_text)

                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É har_and_cookies –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç –∏ –¥–∞–µ–º –ø—Ä–∞–≤–∞
                os.makedirs("har_and_cookies", exist_ok=True)
                os.chmod("har_and_cookies", 0o755)  # –ü—Ä–∞–≤–∞ –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
                    cleanup_image(img_path)
                    
                    return
                 
                model_answer = completion.choices[0].message.content
                model_new_answer = clean_output(clean_markdown(model_answer))

                await enable_message.delete()
                await send_long_message(model_new_answer, message)

                return

            except Exception as e:
                logging.warning(f"Attempt {attempt} failed: {str(e)}")

                if attempt == len(all_models):
                    await message.answer('‚ö†Ô∏è ***–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏***', parse_mode="MARKDOWN")
    
        cleanup_image(img_path)
    
    except Exception as e:
        logging.error(f"Error in handle_model_request: {str(e)}")
        await message.answer('‚ùå ***–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...***', parse_mode="MARKDOWN")

    finally:
        cleanup_image(img_path)









@dp.message(Command('start'))
async def start(message: Message):
    try:
        db.create_tables()
        black_photo_path = 'fotos/black_img.jpg'

        if (not db.user_exists(message.from_user.id)):
            db.add_user(message.from_user.id)
            db.set_nickname(message.from_user.id, message.from_user.username)
            db.set_signup(message.from_user.id, 'done')

        await message.answer_photo(photo=FSInputFile(black_photo_path),
                                   caption=f'–ü—Ä–∏–≤–µ—Ç, {message.from_user.first_name}. –Ø AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ Telegram. –ó–¥–µ—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∏–µ –∫–∞–∫: ***GPT, DEEPSEEK, GEMIMI –∏ –¥—Ä—É–≥–∏–µ.*** \n\n'
                                           f'***–¢—ã –º–æ–∂–µ—à—å –≤—ã–±—Ä–∞—Ç—å —É–¥–æ–±–Ω—É—é –¥–ª—è —Å–µ–±—è –º–æ–¥–µ–ª—å –ø–æ –∫–Ω–æ–ø–∫–µ.*** üëá',
                                   parse_mode="MARKDOWN", reply_markup=main_keyboard())

    except Exception as e:
        print(e)





@dp.message()
async def get_message(message: Message):
    try:
        current_model = db.get_model(message.from_user.id)

        img_path = None
        if message.photo:
            img_path = f'user_photos/{message.photo[-1].file_id}.jpg'
            await download_photo(message.photo[-1].file_id, img_path)


        new_api = ''
        model_title = ''
        for model, api in allowed_models.items():
            if api['code'] == current_model:
                new_api = api['api-key']
                model_title = model
                break

        if not new_api or not current_model:
            raise ValueError(f"API key not found for model: {current_model}")


        client = OpenAI(api_key=f"{new_api}", base_url="https://openrouter.ai/api/v1")
        gpt_client = Client()


        # DEEPSEEK FAMILY
        # DEEPSEEK R1 requests
        if current_model == 'deepseek-r1':
            await handle_model_requests(message,
                                        img_support=False,
                                        img_path=img_path,
                                        all_models=[
                                        {'model': 'deepseek/deepseek-r1', 'client': client},
                                        {'model': g4f.models.deepseek_r1, 'client': gpt_client}
                                        ],
                                        model_title=model_title,
                                        system_prompt=system_prompt)





        # DEEPSEEK V3
        if current_model == 'deepseek-v3':
            await handle_model_requests(message,
                                        img_support=False,
                                        img_path=img_path,
                                        all_models=[
                                        {'model': 'deepseek/deepseek-chat-v3-0324', 'client': client},
                                        {'model': g4f.models.deepseek_v3, 'client': gpt_client}
                                        ],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # DEEPSEK r1 (qwen)
        if current_model == 'deepseek-r1-qwen':
            await handle_model_requests(message,
                                        img_support=False,
                                        img_path=img_path,
                                        all_models=[
                                        {'model': 'deepseek/deepseek-r1-distill-qwen-32b', 'client': client},
                                        {'model': g4f.models.deepseek_r1_distill_qwen_32b, 'client': gpt_client}
                                        ],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # GPT 4 turbo
        if current_model == 'gpt4-turbo':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'gpt-4-turbo', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # GPT 4.1
        if current_model == 'gpt4.1':     
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'gpt-4.1', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # GPT 4o
        if current_model == 'gpt4-o':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'gpt-4o', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # GPT 4.1 mini
        if current_model == 'gpt4.1-mini':
            await handle_model_requests(message,
                                        img_support=False,
                                        img_path=img_path,
                                        all_models=[{'model': 'gpt-4.1-mini', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # GPT 4o MINI
        if current_model == 'gpt4-o-mini':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'gpt-4o-mini', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)


        # CLAUDE family
        # CLAUDE 3.7 sonnet
        if current_model == 'claude3.7-sonnet':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'claude-3.7-sonnet', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)


        # CLAUDE 3.7 sonnet (thinking)
        if current_model == 'claude3.7-sonnet-thinking':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'claude-3.7-sonnet-thinking', 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # OPEN AI FAMILY (o3)
        if current_model == 'open-ai-o3':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': 'openai/o3', 'client': client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # OPEN AI O4 mini
        if current_model == 'open-ai-o4-mini':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[{'model': g4f.models.o4_mini, 'client': gpt_client}],
                                        model_title=model_title,
                                        system_prompt=system_prompt)


        # QWEN FAMILY (235)
        if current_model == 'qwen3-235B-A22B':
            await handle_model_requests(message,
                                        img_support=False,
                                        img_path=img_path,
                                        all_models=[
                                        {'model': 'qwen-3-235b', 'client': gpt_client},
                                        {'model': 'qwen/qwen3-235b-a22b', 'client': client}
                                        ],
                                        model_title=model_title,
                                        system_prompt=system_prompt)

        # QWEN 30b
        if current_model == 'qwen3-30b-a3b':
            await handle_model_requests(message,
                                        img_support=False,
                                        img_path=img_path,
                                        all_models=[
                                        {'model': 'qwen-3-30b', 'client': gpt_client},
                                        {'model': 'qwen/qwen3-30b-a3b', 'client': client}
                                        ],
                                        model_title=model_title,
                                        system_prompt=system_prompt)


        # GEMINI FAMILY (2.0 flash lite)
        if current_model == 'gemini-2.0-flash-lite':
            await handle_model_requests(message,
                                        img_support=True,
                                        img_path=img_path,
                                        all_models=[
                                        {'model': g4f.models.gemini_2_0_flash_thinking, 'client': gpt_client}
                                        ],
                                        model_title=model_title,
                                        system_prompt=system_prompt)




    except Exception as e:
        print(e)
        await message.answer('‚ùå ***–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...***', parse_mode='MARKDOWN')
        return








@dp.callback_query(lambda F: True)
async def change_model(callback_query: CallbackQuery):
    black_photo_path = 'fotos/black_img.jpg'

    try:
        if callback_query.data == 'change_model':
            builder = InlineKeyboardBuilder()
            cur_model = db.get_model(callback_query.from_user.id)


            for model_name, model_data in allowed_models.items():
                if model_data['code'] == cur_model:
                    builder.button(text=f"‚úÖ {model_name}", callback_data=f"model_{model_data['code']}")
                else:
                    builder.button(text=model_name, callback_data=f"model_{model_data['code']}")


            builder.adjust(3, 1, 1, 1, 1, 1, 2, 2, 2)

            await callback_query.message.answer(f'–í —Ä–∞–∑–¥–µ–ª–µ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∏–µ –∫–∞–∫ <b>ChatGPT, Claude, Gemini, Deepseek –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ</b>:\n\n'
                                               f'<b>üêº Deepseepk-R1</b> - –ú–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Å –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º\n'
                                               f'<b>üê≥ Deepseek-V3</b> - –ö–∏—Ç–∞–π—Å–∫–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–æ–∑–¥–∞–Ω–∞—è –õ—è–Ω–æ–º –í—ç–Ω—å—Ñ—ç–Ω–æ–º\n'
                                               f'<b>‚ö° Deepseek-QWEN</b> - Deepseek –Ω–∞ –±–∞–∑–µ –∫–∏—Ç–∞–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏ QWEN\n\n'
                                               f'<b>üçì OpenAI-O3</b> - –†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–ª—É—á—à–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏\n'
                                               f'<b>üß† OpenAI-O4 mini</b> - –î–ª—è –∫–æ–¥–∏–Ω–≥–∞ –∏ —Ç–æ—á–Ω—ã—Ö –Ω–∞—É–∫\n\n'
                                               f'<b>‚ú® GPT-4 Turbo</b> ‚Äì –ú–æ—â–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å OpenAI —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.\n'
                                               f'<b>üí• GPT-4.1</b> ‚Äì –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-4 —Å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.\n'
                                               f'<b>üíé GPT-4o</b> ‚Äì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏—è GPT-4.\n'
                                               f'<b>üçÉ GPT-4.1 Mini</b> ‚Äì –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-4.1.\n\n'
                                               f'<b>üîÆ Claude 3.7 Sonnet</b> ‚Äì –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Anthropic —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.\n'
                                               f'<b>‚òÅ Claude 3.7 Sonnet (thinking)</b> ‚Äì –í–µ—Ä—Å–∏—è —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" –¥–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n\n'
                                               f'<b>üí¨ Qwen3 235B A22B</b> ‚Äì –ú–∞—Å—à—Ç–∞–±–Ω–∞—è –º–æ–¥–µ–ª—å Qwen —Å 235 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.\n'
                                               f'<b>ü§ñ Qwen3 30B A3B</b> ‚Äì –ë–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è Qwen3, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞.\n\n'
                                               f'<b>üí° Gemini 2.0 Flash Lite</b> ‚Äì –û–±–ª–µ–≥—á—ë–Ω–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è Gemini 2.0, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.',
                                       parse_mode="html", reply_markup=builder.as_markup())


        if callback_query.data.startswith('model_'):
            new_model = callback_query.data.replace('model_', '')
            db.set_model(callback_query.from_user.id, new_model)

            await update_keyboard(callback_query.message, callback_query.from_user.id)


    except Exception as e:
        print(e)





# POLLING
async def main():
    await dp.start_polling(bot)

if '__main__' == __name__:
    asyncio.run(main())
