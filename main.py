import os
from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.types import Message, FSInputFile, CallbackQuery
from aiogram.utils.keyboard import InlineKeyboardBuilder, InlineKeyboardButton
from aiogram.enums import ParseMode


from keyboards import main_keyboard
from types import MappingProxyType

import asyncio
from openai import OpenAI
from g4f.client import Client
from g4f.Provider import You, DeepInfra
import g4f
import re
from db import Database
from dotenv import load_dotenv
import base64


load_dotenv()
env = os.getenv


bot = Bot(env("BOT_TOKEN"))
dp = Dispatcher(storage=MemoryStorage())
db = Database('database.db')



# DEEPSEEK SETTINGS (Deepseek-r1 qwen 32b) and other models



system_prompt = """ü§ñ‚ú® You are an expert multilingual AI assistant and developer with extensive experience. Follow these advanced guidelines:

 üåê 1. Language Processing (Intelligent Multilingual Handling) üß†
     - üîç Perform 3-step language analysis:
       1Ô∏è‚É£ 1. Detect primary language using linguistic patterns üïµÔ∏è‚ôÇÔ∏è
       2Ô∏è‚É£ 2. Identify secondary languages if code-mixing exceeds 30% üåç
       3Ô∏è‚É£ 3. Recognize technical terms that should remain untranslated ‚öôÔ∏è
     - üì¢ Response language mirroring:
       * üéØ Match the user's primary language with 98% accuracy
       * üîí Preserve original terminology for: proper nouns, technical terms, cultural concepts
       * üåà For mixed input (e.g., Hinglish, Spanglish), maintain the dominant language base

 üìù 2. Advanced Response Formatting (Structured & Precise) üé®
     - üóÇ Apply hierarchical organization:
       ‚Ä¢ üöÄ **<concise 15-word summary>**
       ‚Ä¢ üìå Supporting arguments (bullet points)
       ‚Ä¢ üíª Examples (indented code blocks if technical)
       ‚Ä¢ üåç Cultural/localization notes (italic when relevant)
     - ‚è± Strict length management:
       * üìè Real-time character count including Markdown (max 4096)
       * ‚úÇÔ∏è Auto-truncation algorithm:
         - üîÑ Preserve complete sentences
         - üéØ Prioritize core information
         - ‚ûï Add "[...]" if truncated
     - üé≠ Important style work (other Markdown and emojis):
       * üòä Use 3-5 relevant emojis per response section
       * üîÄ Use different fonts (MARKDOWN + EMOJI combinations)

 üíº 3. Specialized Content Handling ‚öôÔ∏è
     - üë®üíª Technical material:
       > üîß Maintain original English terms with localized explanations
       > üíª Use ```code blocks``` for all commands/APIs
     - üåè Cultural adaptation:
       * üìè Adjust measurements (metric/imperial)
       * üí∞ Localize examples (currency, idioms)
       * üö® Recognize region-specific sensitivities

 ‚úÖ 4. Quality Assurance Protocols üîç
     - üîÑ Run pre-response checks:
       1. üìö Language consistency validation
       2. üìä Information density audit
       3. üåê Cultural appropriateness scan
     - üßê Post-generation review:
       * ‚úîÔ∏è Verify factual accuracy
       * üéö Ensure tone alignment (professional ‚Üí friendly spectrum)
       * üìñ Confirm readability score >80%

 üì§ Output template:
   ‚ú® **Title/Subject (if applicable)**
   
   ‚Ä¢ üéØ Key point 1
   
   ‚Ä¢ üîë Key point 2
   
   - üìç Supporting detail
   
   - üí° Example/excerpt
   
   üåü Additional tip (optional)
   
 üåç <cultural/localization note if relevant>



"""



allowed_models = MappingProxyType({
    # DEEPSEEK family
    'Deepseek-R1': {
        'code': 'deepseek-r1',
        'api-key': env("DEEPSEEK_API_R1"),
    },
    'Deepseek-V3': {
        'code': 'deepseek-v3',
        'api-key': env("DEEPSEEK_API_V3"),
    },
    'Deepseek-R1 (QWEN)': {
        'code': 'deepseek-r1-qwen',
        'api-key': env("DEEPSEEK_API_QWEN"),
    },


   # GPT family
   'GPT-4 Turbo': {
        'code': 'gpt4-turbo',
        'api-key': env("GPT_4_TURBO_API"),
   },
   'GPT-4.1': {
        'code': 'gpt4.1',
        'api-key': env("GPT_4_1_API"),
   },
   'GPT-4o': {
       'code': 'gpt4-o',
       'api-key': env("GPT_4_O_API"),
   },

   # MINI GPT`s family
   'GPT-4.1 Mini': {
       'code': 'gpt4.1-mini',
       'api-key': env("GPT_4_1_MINI_API"), 
   },
   'GPT-4o Mini': {
       'code': 'gpt4-o-mini',
       'api-key': env("GPT_4_O_MINI_API"),
   },

   # CLAUDE family
   'Claude 3.7 Sonnet': {
       'code': 'claude3.7-sonnet',
       'api-key': env("CLAUDE_37_API"),
   },
   'Claude 3.7 Sonnet (thinking)': {
       'code': 'claude3.7-sonnet-thinking',
       'api-key': env("CLAUDE_37_TH_API"),
   },

   # Open AI family
   'OpenAI o3': {
       'code': 'open-ai-o3',
       'api-key': env("OAI_O3"),
   },
   'Open AI o4 Mini': {
       'code': 'open-ai-o4-mini',
       'api-key': env("OAI_O4_MINI"),
   },


   # QWEN family
   'Qwen3 235B A22B': {
       'code': 'qwen3-235B-A22B',
       'api-key': env("QWEN_3_235"),
   },
   'Qwen3 30B A3B': {
       'code': 'qwen3-30b-a3b',
       'api-key': env("QWEN_3_30"),
   },



   # Gemini family
   'Gemini 2.0 Flash Lite': {
       'code': 'gemini-2.0-flash-lite',
       'api-key': env("GEMINI_API"),
   },
})




def format_answer(answer: str) -> str:
    # –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—É–Ω–∫—Ç–∞–º
    return "\n\n".join(answer.split('\n'))

def clean_output(text):
    return re.sub(r'\\boxed\{([^}]*)\}', r'\1', text)


async def download_photo(file_id: str, path: str):
    file = await bot.get_file(file_id)
    await bot.download_file(file.file_path, path)


def clean_markdown(text: str) -> str:
    patterns = [
        # (r'```.*?\n(.*?)\n```', r'\1', re.DOTALL), –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –∫–æ–¥ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        # (r'`(.*?)`', r'\1'),
        (r'\*\*(.*?)\*\*', r'*\1*'),  # –ñ–∏—Ä–Ω—ã–π ‚Üí –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π Markdown
        (r'^#+\s*(.+)$', r'*\1*', re.MULTILINE),  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ ‚Üí –∂–∏—Ä–Ω—ã–π
    ]

    for pattern in patterns:
        if len(pattern) == 3:
            p, r, f = pattern
            text = re.sub(p, r, text, flags=f)
        else:
            p, r = pattern
            text = re.sub(p, r, text)

    return text


async def update_keyboard(message: Message, user_id: int):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    current_model = db.get_model(user_id)
    builder = InlineKeyboardBuilder()

    for name, data in allowed_models.items():
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–∞–ª–æ—á–∫—É –∫ —Ç–µ–∫—É—â–µ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        is_selected = (data['code'] == current_model)
        builder.button(
            text=f"{'‚úÖ ' if is_selected else ''}{name}",
            callback_data=f"model_{data['code']}"
        )

    builder.adjust(3, 1, 1, 1, 1, 1, 2, 2, 2)
    await message.edit_reply_markup(reply_markup=builder.as_markup())



async def create_response(model,
                          prompt: str,
                          text: str,
                          client,
                          temperature: float = 0.7,
                          top_p: float = 0.9,
                          fp: float = 0.2,
                          presence_penalty: float = 0.2,
                          max_tokens: int | None=None,
                          img_path: str | None=None,
                          provider=None):
    if img_path:
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Img Path NOT Found: {img_path}")
        
        if not os.access(img_path, os.R_OK):
            raise PermissionError(f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_path}")

        with open(img_path, "rb") as f:
            image_b64 = base64.b64encode(f.read()).decode("utf-8")
            
        completion = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": prompt},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": text},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
                    ],
                },
            ],
            provider=provider,
            temperature=temperature,
            top_p=top_p,
            frequency_penalty=fp,
            presence_penalty=presence_penalty,
            max_tokens=max_tokens
        )
        return completion

    
    completion = client.chat.completions.create(
        extra_body={},
        model=model,
        messages=[
            {
                "role": "system",
                "content": prompt
            },
            {
                "role": "user",
                "content": text
            }
        ],
        provider=provider,
        temperature=temperature,  # –ö–æ–Ω—Ç—Ä–æ–ª—å "–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏" (0‚Äì1)
        top_p=top_p,  # –í–ª–∏—è–µ—Ç –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
        frequency_penalty=fp,  # –£–º–µ–Ω—å—à–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        presence_penalty=presence_penalty,  # –ü–æ–æ—â—Ä—è–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–º—ã
        max_tokens=max_tokens
    )
    return completion









@dp.message(Command('start'))
async def start(message: Message):
    try:
        db.create_tables()
        black_photo_path = 'fotos/black_img.jpg'

        if (not db.user_exists(message.from_user.id)):
            db.add_user(message.from_user.id)
            db.set_nickname(message.from_user.id, message.from_user.username)
            db.set_signup(message.from_user.id, 'done')

        await message.answer_photo(photo=FSInputFile(black_photo_path),
                                   caption=f'–ü—Ä–∏–≤–µ—Ç, {message.from_user.first_name}. –Ø AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ Telegram. –ó–¥–µ—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∏–µ –∫–∞–∫: ***GPT, DEEPSEEK, GEMIMI –∏ –¥—Ä—É–≥–∏–µ.*** \n\n'
                                           f'***–¢—ã –º–æ–∂–µ—à—å –≤—ã–±—Ä–∞—Ç—å —É–¥–æ–±–Ω—É—é –¥–ª—è —Å–µ–±—è –º–æ–¥–µ–ª—å –ø–æ –∫–Ω–æ–ø–∫–µ.*** üëá',
                                   parse_mode="MARKDOWN", reply_markup=main_keyboard())

    except Exception as e:
        print(e)





@dp.message()
async def get_message(message: Message):
    async def send_long_message(text, message):
        if len(text) <= 4096:
            await message.answer(text, parse_mode='MARKDOWN')
        else:
            parts = [text[i:i+4096] for i in range(0, len(text), 4096)]
            for part in parts:
                await message.answer(part, parse_mode='MARKDOWN')


    try:
        current_model = db.get_model(message.from_user.id)

        img_path = None
        if message.photo:
            img_path = f'user_photos/{message.photo[-1].file_id}.jpg'
            await download_photo(message.photo[-1].file_id, img_path)


        new_api = ''
        model_title = ''
        for model, api in allowed_models.items():
            if api['code'] == current_model:
                new_api = api['api-key']
                model_title = model
                break

        if not new_api or not current_model:
            raise ValueError(f"API key not found for model: {current_model}")


        client = OpenAI(api_key=f"{new_api}", base_url="https://openrouter.ai/api/v1")
        gpt_client = Client()




        # DEEPSEEK R1 requests
        if current_model == 'deepseek-r1':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***', parse_mode="MARKDOWN")

            # –ü–û–ü–´–¢–ö–ê 1
            try:
                completion = await create_response(model='deepseek/deepseek-r1', prompt=system_prompt,
                                                   text=message.text, client=client)

                if img_path and os.path.exists(img_path):
                    os.remove(img_path)
                
                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

                
                deepseek_answer = completion.choices[0].message.content
                new_deepseek_answer = clean_output(clean_markdown(deepseek_answer))

                await enable_message.delete()
                await send_long_message(new_deepseek_answer, message)
                print('deepseek 1')


            # –ü–û–ü–´–¢–ö–ê 2
            except Exception:
                completion = await create_response(model=g4f.models.deepseek_r1, prompt=system_prompt, text=message.text, client=gpt_client)

                
                if img_path and os.path.exists(img_path):
                    os.remove(img_path)
               

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

                deepseek_answer = completion.choices[0].message.content

                print(deepseek_answer)
                print(clean_markdown(deepseek_answer))
                print(len(clean_output(clean_markdown(deepseek_answer))))

                new_deepseek_answer = clean_output(clean_markdown(deepseek_answer))

            # if '`' in new_deepseek_answer[0:2] and '`' in new_deepseek_answer[-3:-1]:
            #     if len([char for char in new_deepseek_answer]) >= 4096:
            #         while new_deepseek_answer:
            #             await message.answer(new_deepseek_answer[:4096], parse_mode='MARKDOWN')
            #             # —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
            #             new_deepseek_answer = new_deepseek_answer[4096:]
            #     await message.answer(new_deepseek_answer[2:-3], parse_mode='MARKDOWN')
            #
                await enable_message.delete()
                await send_long_message(new_deepseek_answer, message)
                print('deepseek 2')




        # DEEPSEEK FAMILY
        if current_model == 'deepseek-v3':
            enable_message = await message.answer(f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                                 parse_mode="MARKDOWN")
            try:
                completion = await create_response(model=g4f.models.deepseek_v3, text=message.text, prompt=system_prompt, client=gpt_client)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

                deepseek_answer = completion.choices[0].message.content
                new_deepseek_answer = clean_output(clean_markdown(deepseek_answer))
                
                await enable_message.delete()
                await send_long_message(new_deepseek_answer, message)


            except Exception as e: 
                completion = await create_response(model='deepseek/deepseek-chat-v3-0324', text=message.text, prompt=system_prompt, client=client)
                deepseek_answer = completion.choices[0].message.content


                print(deepseek_answer)
                print(clean_markdown(deepseek_answer))
                print(len(clean_output(clean_markdown(deepseek_answer))))
                new_deepseek_answer = clean_output(clean_markdown(deepseek_answer))


                await enable_message.delete()
                await send_long_message(new_deepseek_answer, message)







        # DEEPSEK r1 (qwen)
        if current_model == 'deepseek-r1-qwen':
            enable_message = await message.answer(f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                                 parse_mode="MARKDOWN")

            try:
                completion = await create_response(model=g4f.models.deepseek_r1_distill_qwen_32b, text=message.text, prompt=system_prompt, client=gpt_client)
                
                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

                deepseek_answer = completion.choices[0].message.content
                new_deepseek_answer = clean_output(clean_markdown(deepseek_answer))

                await enable_message.delete()
                await send_long_message(new_deepseek_answer, message)

            except Exception:
                completion = await create_response(model='deepseek/deepseek-r1-distill-qwen-32b', text=message.text, prompt=system_prompt, client=client)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
        
                deepseek_answer = completion.choices[0].message.content
                new_deepseek_answer = clean_output(clean_markdown(deepseek_answer))

                await enable_message.delete()
                await send_long_message(new_deepseek_answer, message)







        # GPT
        if current_model == 'gpt4-turbo':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")
            completion = await create_response(model='gpt-4-turbo', text=message.text, prompt=system_prompt, client=gpt_client)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
        
            gpt_answer = completion.choices[0].message.content
            new_gpt_answer = clean_output(clean_markdown(gpt_answer))

            await enable_message.delete()
            await send_long_message(gpt_answer, message)






        if current_model == 'gpt4.1':            
            try:
                print('–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–∞', flush=True)
                
                enable_message = await message.answer(
                    f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                    parse_mode="MARKDOWN")
                
                message_text = message.caption if message.photo else message.text

                print(f'–°–æ–æ–±—â–µ–Ω–∏–µ "{message_text}", —Ç–∏–ø: {type(message_text)}', flush=True)


                if isinstance(message_text, list):
                    message_text = " ".join(message_text)


                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É har_and_cookies –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç –∏ –¥–∞–µ–º –ø—Ä–∞–≤–∞
                os.makedirs("har_and_cookies", exist_ok=True)
                os.chmod("har_and_cookies", 0o755)  # –ü—Ä–∞–≤–∞ –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å


                completion = await create_response(text=message_text, client=gpt_client, model='gpt-4.1', prompt=system_prompt, img_path=img_path)

                if img_path and os.path.exists(img_path):
                    os.remove(img_path)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

                gpt_answer = clean_output(clean_markdown(completion.choices[0].message.content))
                await enable_message.delete()


                await send_long_message(gpt_answer, message)
            
            except PermissionError as e:
                print(f'‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {str(e)}')
    
            except Exception as e:
                print(f'‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}')









        if current_model == 'gpt4-o':
            try:
                enable_message = await message.answer(
                    f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                    parse_mode="MARKDOWN")


                message_text = message.caption if message.photo else message.text

                print(f'–°–æ–æ–±—â–µ–Ω–∏–µ "{message_text}", —Ç–∏–ø: {type(message_text)}', flush=True)

                if isinstance(message_text, list):
                    message_text = " ".join(message_text)


                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É har_and_cookies –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç –∏ –¥–∞–µ–º –ø—Ä–∞–≤–∞
                os.makedirs("har_and_cookies", exist_ok=True)
                os.chmod("har_and_cookies", 0o755)  # –ü—Ä–∞–≤–∞ –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å

                completion = await create_response(model='gpt-4o', prompt=system_prompt, client=gpt_client, text=message_text, img_path=img_path)

                if img_path and os.path.exists(img_path):
                    os.remove(img_path)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

                gpt_answer = clean_output(clean_markdown(completion.choices[0].message.content))
                await enable_message.delete()

                await send_long_message(gpt_answer, message)

            except PermissionError as e:
                print(f'‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {str(e)}')
    
            except Exception as e:
                print(f'‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}')




        if current_model == 'gpt4.1-mini':
            if message.photo:
                if img_path and os.path.exists(img_path):
                    os.remove(img_path)

                await message.answer('–ù–ï–¢ –ü–û–î–î–ï–†–ñ–ö–ò –§–û–¢–û')
                return

            enable_message = await message.answer(
                    f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                    parse_mode="MARKDOWN")

            message_text = message.text

            if isinstance(message_text, list):
                message_text = " ".join(message_text)

            
            completion = await create_response(text=message_text, model='gpt-4.1-mini', prompt=system_prompt, client=gpt_client)


            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

            gpt_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()

            await send_long_message(gpt_answer, message)







        if current_model == 'gpt4-o-mini':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            completion = await create_response(text=message.text, model='gpt-4o-mini', client=gpt_client, prompt=system_prompt)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

            gpt_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()

            await send_long_message(gpt_answer, message)








        # CLAUDE family
        if current_model == 'claude3.7-sonnet':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            completion = await create_response(text=message.text, model=g4f.models.claude_3_7_sonnet, client=gpt_client, prompt=system_prompt)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')


            claude_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()

            await send_long_message(claude_answer, message)







        if current_model == 'claude3.7-sonnet-thinking':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            completion = await create_response(text=message.text, client=gpt_client, model=g4f.models.claude_3_7_sonnet_thinking, prompt=system_prompt)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')


            claude_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()

            await send_long_message(claude_answer, message)






        # OPEN AI FAMILY
        if current_model == 'open-ai-o3':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            completion = await create_response(text=message.text, client=client, model='openai/o3', prompt=system_prompt)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

            oai_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()

            await send_long_message(oai_answer, message)





        if current_model == 'open-ai-o4-mini':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            completion = await create_response(text=message.text, client=gpt_client, model=g4f.models.o4_mini, prompt=system_prompt)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

            oai_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()

            await send_long_message(oai_answer, message)




        # QWEN FAMILY
        if current_model == 'qwen3-235B-A22B':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            try:
                completion = await create_response(text=message.text, client=client, model='qwen/qwen3-235b-a22b', prompt=system_prompt)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
        
                qwen_answer = clean_output(clean_markdown(completion.choices[0].message.content))
                await enable_message.delete()

                await send_long_message(qwen_answer, message)
                print('qwen 1')
            
            except Exception:
                completion = await create_response(text=message.text, client=gpt_client, model='qwen-3-235b', prompt=system_prompt)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
        
                qwen_answer = clean_output(clean_markdown(completion.choices[0].message.content))
                await enable_message.delete()

                await send_long_message(qwen_answer, message)






        if current_model == 'qwen3-30b-a3b':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            try:
                completion = await create_response(text=message.text, client=client, model='qwen/qwen3-30b-a3b', prompt=system_prompt)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
        
                qwen_answer = clean_output(clean_markdown(completion.choices[0].message.content))
                await enable_message.delete()

                await send_long_message(qwen_answer, message)
            
            except Exception:
                completion = await create_response(text=message.text, client=gpt_client, model='qwen-3-30b', prompt=system_prompt)

                if not completion.choices:
                    await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')
        
                qwen_answer = clean_output(clean_markdown(completion.choices[0].message.content))
                await enable_message.delete()

                await send_long_message(qwen_answer, message)



        # GEMINI FAMILY
        if current_model == 'gemini-2.0-flash-lite':
            enable_message = await message.answer(
                f'üõ†Ô∏è ***–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ, {model_title} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å...***',
                parse_mode="MARKDOWN")

            completion = await create_response(text=message.text, client=gpt_client, model=g4f.models.gemini_2_0_flash_thinking, prompt=system_prompt)

            if not completion.choices:
                await message.answer(f'‚ùå ***{model_title} –Ω–∏—á–µ–≥–æ –Ω–µ –≤–µ—Ä–Ω—É–ª***', parse_mode='MARKDOWN')

            gemini_answer = clean_output(clean_markdown(completion.choices[0].message.content))
            await enable_message.delete()
            
            await send_long_message(gemini_answer, message)




    except Exception as e:
        print(e)
        await message.answer('‚ùå ***–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...***', parse_mode='MARKDOWN')








@dp.callback_query(lambda F: True)
async def change_model(callback_query: CallbackQuery):
    black_photo_path = 'fotos/black_img.jpg'

    try:
        if callback_query.data == 'change_model':
            builder = InlineKeyboardBuilder()
            cur_model = db.get_model(callback_query.from_user.id)


            for model_name, model_data in allowed_models.items():
                if model_data['code'] == cur_model:
                    builder.button(text=f"‚úÖ {model_name}", callback_data=f"model_{model_data['code']}")
                else:
                    builder.button(text=model_name, callback_data=f"model_{model_data['code']}")


            builder.adjust(3, 1, 1, 1, 1, 1, 2, 2, 2)

            await callback_query.message.answer(f'–í —Ä–∞–∑–¥–µ–ª–µ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∏–µ –∫–∞–∫ <b>ChatGPT, Claude, Gemini, Deepseek –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ</b>:\n\n'
                                               f'<b>üêº Deepseepk-R1</b> - –ú–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Å –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º\n'
                                               f'<b>üê≥ Deepseek-V3</b>* - –ö–∏—Ç–∞–π—Å–∫–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–æ–∑–¥–∞–Ω–∞—è –õ—è–Ω–æ–º –í—ç–Ω—å—Ñ—ç–Ω–æ–º\n'
                                               f'<b>‚ö° Deepseek-QWEN</b> - Deepseek –Ω–∞ –±–∞–∑–µ –∫–∏—Ç–∞–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏ QWEN\n\n'
                                               f'<b>üçì OpenAI-O3</b> - –†–∞—Å—Å—É–∂–¥–∞—é—â–∞—è –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–ª—É—á—à–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏\n'
                                               f'<b>üß† OpenAI-O4 mini</b> - –î–ª—è –∫–æ–¥–∏–Ω–≥–∞ –∏ —Ç–æ—á–Ω—ã—Ö –Ω–∞—É–∫\n\n'
                                               f'<b>‚ú® GPT-4 Turbo</b> ‚Äì –ú–æ—â–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å OpenAI —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.\n'
                                               f'<b>üí• PT-4.1</b> ‚Äì –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-4 —Å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.\n'
                                               f'<b>üíé GPT-4o</b> ‚Äì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏—è GPT-4.\n'
                                               f'<b>üçÉ GPT-4.1 Mini</b> ‚Äì –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-4.1.\n\n'
                                               f'<b>üîÆ Claude 3.7 Sonnet</b> ‚Äì –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Anthropic —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.\n'
                                               f'<b>‚òÅ Claude 3.7 Sonnet (thinking)</b> ‚Äì –í–µ—Ä—Å–∏—è —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" –¥–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n\n'
                                               f'<b>üí¨ Qwen3 235B A22B</b> ‚Äì –ú–∞—Å—à—Ç–∞–±–Ω–∞—è –º–æ–¥–µ–ª—å Qwen —Å 235 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.\n'
                                               f'<b>ü§ñ Qwen3 30B A3B</b> ‚Äì –ë–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è Qwen3, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞.\n\n'
                                               f'<b>üí° Gemini 2.0 Flash Lite</b> ‚Äì –û–±–ª–µ–≥—á—ë–Ω–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è Gemini 2.0, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.',
                                       parse_mode="html", reply_markup=builder.as_markup())


        if callback_query.data.startswith('model_'):
            new_model = callback_query.data.replace('model_', '')
            db.set_model(callback_query.from_user.id, new_model)

            await update_keyboard(callback_query.message, callback_query.from_user.id)


    except Exception as e:
        print(e)





# POLLING
async def main():
    await dp.start_polling(bot)

if '__main__' == __name__:
    asyncio.run(main())
